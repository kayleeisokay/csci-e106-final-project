---
title: "HARVARD EXTENSION SCHOOL"
subtitle: "CSCI E-106 Class Group Project"
author:
- Will Greaves
- Flora Lo
- Matt Michel
- Thaylan Toth
- Kaylee Vo
- Zhenzhen Yin
tags: [logistic, neural networks, regression trees]
abstract: |
  We aimed to develop models for predicting house prices in Kings County, USA using statistical modeling and machine learning approaches. Our dataset contained historical home sales prices of 21,613 houses in Kings County, USA (May 2014-May 2015), out of which we randomly sampled 70% for training and 30% for testing. We selected several significant features using feature selection methods to build the models. Seven different linear regression models were developed using R and compared against each other, with lasso regression achieving the highest adjusted R-squared (0.775). In addition, we developed alternative models using regression trees, which achieved an adjusted R-squared value of 0.65.
  
  In conclusion, we proposed a model that could predict house sales prices based on commonly measured variables. We believe such a model can serve as a helpful tool for prospective consumers and real estate service providers to estimate the value of future properties on the market, and for them to understand significant factors that may increase or decrease home values. Importantly, we expect this model to be valid only within the geographical region of King County and for a limited period of time into the future. It is subject to changes in the market and broader economic conditions, thus we also provided a detailed model monitoring plan to alert us of significant deviations from our model. 
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: true
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
linkcolor: blue
urlcolor: blue
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newpage

# Prologue
## Data Dictionary - House Sales in King County, USA

| Variable| Description |
| :-------:| :------- |
| id| **Unique ID for each home sold (it is not a predictor)**    |
| date| *Date of the home sale*    |
| price| *Price of each home sold*    |
| bedrooms| *Number of bedrooms*    |
| bathrooms| *Number of bathrooms, where ".5" accounts for a bathroom with a toilet but no shower*    |
| sqft_living| *Square footage of the apartment interior living space*    |
| sqft_lot| *Square footage of the land space*    |
| floors| *Number of floors*    |
| waterfront| *A dummy variable for whether the apartment was overlooking the waterfront or not*    |
| view| *An index from 0 to 4 of how good the view of the property was*    |
| condition| *An index from 1 to 5 on the condition of the apartment,*    |
| grade| *An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 has a high-quality level of construction and design.*    |
| sqft_above| *The square footage of the interior housing space that is above ground level*    | 
| sqft_basement| *The square footage of the interior housing space that is below ground level*    |
| yr_built| *The year the house was initially built*    |
| yr_renovated| *The year of the house's last renovation*    |
| zipcode| *What zipcode area the house is in*    |
| lat| *Latitude*    |
| long| *Longitude*    |
| sqft_living15| *The square footage of interior housing living space for the nearest 15 neighbors*    |
| sqft_lot15| *The square footage of the land lots of the nearest 15 neighbors*    |
\newpage

```{r, include = FALSE}
## Instructions:
# 0.  Join a team with your fellow students with appropriate size (Four Students total)
# 1.  Load and Review the dataset named "KC_House_Sales'csv
# 2.	Create the train data set which contains 70% of the data and use set.seed (1023). The remaining 30% will be your test data set.
# 3.	Investigate the data and combine the level of categorical variables if needed and drop variables as needed. For example, you can drop id, Latitude, Longitude, etc.
# 4.	Build a regression model to predict price. 
# 5.	Create scatter plots and a correlation matrix for the train data set. Interpret the possible relationship between the response.
# 6.	Build the best multiple linear models by using the stepwise selection method. Compare the performance of the best two linear models. 
# 7.	Make sure that model assumption(s) are checked for the final model. Apply remedy measures (transformation, etc.) that helps satisfy the assumptions. 
# 8.	Investigate unequal variances and multicollinearity. If necessary, apply remedial methods (WLS, Ridge, Elastic Net, Lasso, etc.). 
# 9.	Build an alternative model based on one of the following approaches to predict price: regression tree, NN, or SVM.  Check the applicable model assumptions. Explore using a logistic regression. 
# 10.	Use the test data set to assess the model performances from above.
# 11.	Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model.
# 12.	Create a model development document that describes the model following this template, input the name of the authors, Harvard IDs, the name of the Group, all of your code and calculations, etc..:
# 
# **Due Date: December 18th, 2023 at 11:59 pm EST**
# 
# **Notes**
# **No typographical errors, grammar mistakes, or misspelled words, use English language**
# **All tables need to be numbered and describe their content in the body of the document**
# **All figures/graphs need to be numbered and describe their content**
# **All results must be accurate and clearly explained for a casual reviewer to fully understand their purpose and impact**
# **Submit both the RMD markdown file and PDF with the sections with appropriate explanations. A more formal document in Word can be used in place of the pdf file but must include all appropriate explanations.**
```


## Executive Summary

In this report, we describe the development of a statistical model that can be used to predict house sales prices in Kings County, USA based on historic house sales data collected between May 2014 and May 2015. Validation in a test data set showed that the best model is significant and has an adjusted R-squared value of 0.775. The model may be applied to estimate property value for consumers and property market agents, as well as to generate insights into key contributing factors to home prices. It is important to note that we expect the model to work well only in geographical locations within King County and within a limited time frame into the future. As such, we have also included a model monitoring plan to detect substantial future deviations of our model.

\newpage
# Section I. Introduction (5 points) {#section1}

```{r, include = FALSE}
# This section needs to introduce the reader to the problem to be resolved, the purpose, and the scope of the statistical testing applied. 
# What you are doing with your prediction? What is the purpose of the model? What methods were trained on the data, how large is the test sample, and how did you build the model?
```


In this project, our goal is to build a statistical model that can predict house sales prices in Kings County, USA based on house sales data collected in that area between May 2014 and May 2015. The dataset contains information on 21613 [houses](#section2), including the sale price, number of rooms, square footage, year built and renovated, view, and condition of the property. The house sale price was used as the outcome variable and all other variables were considered as independent variables. 70% of the dataset was [randomly sampled](#section2c) as training set and the remaining 30% was used as testing set. 

Using this framework, we built several different models using linear regression and regression tree methods. First, we built a baseline ordinary least squares (OLS) model using all available variables. Then we performed stepwise variable selection to ensure inclusion of only significant variables. Diagnostic tests at this point indicated issues with normality assumption, multicollinearity, and constant variance assumption. These issues were largely resolved by a log transformation of the dependent variable as suggested by Box-Cox analysis. We further built models using ridge, lasso, and elastic net regression, as well as a robust regression to account for any effects from outliers. As an additional comparison, we also built a regression tree model and employed hyperparameter tuning procedures. 

Finally, the performance of each model was evaluated on its accuracy in predicting house prices in the test set, specifically by examining the adjusted R-squared and MSE of predicted values. Based on that, we propose that the best predictive model is our lasso regression model (adjusted R-squared = 0.775). Our top performing models indicated that the most important factors that influence house prices in King County, USA are bathrooms, view, grade, latitude, year built, year of purchase, and number of floors. This model may be useful for estimating property prices for home buyers, sellers, or property market professionals. It can also contribute to research on key factors contributing to home prices. Importantly, we expect the model to be valid only in geographical locations within King County and within a limited time frame into the future. Therefore, we have also detailed a model monitoring plan to detect substantial deviations of our model in the future.

\newpage
# Section II. Description of the Data and Quality (15 points) {#section2}

## Year and Month Transformation {#section2a}

```{r, include = FALSE}
# *Here you need to review your data, the statistical test applied to understand the predictors and the response and how are they correlated. Extensive graph analysis is recommended. Is the data continuous, or categorical, do any transformation needed? Do you need dummies? *
```


```{r, message=FALSE}
library(ggplot2)
library(corrplot)
library(readr)
library(dplyr)
library(olsrr)
library(lmtest)
library(nortest)
library(car)
library(glmnet)
```

```{r}
HouseSales <- read.csv("KC_House_Sales.csv")

str(HouseSales)
```

By looking at the structure of the data set, we see variables `date` and `price` are in character format and need to be converted to numeric format. Also, we split the `date` into `year` and `month` and drop the variable `id`, which is not a predictor.

```{r}
# Data cleaning (price)
df = HouseSales
df$price = parse_number(df$price)
```

```{r}
# Transformation (date)
df$year = as.integer(substr(df$date, 1, 4))
df$month = as.integer(substr(df$date, 5, 6))

#store as separate variables for downstream use
year = df$year
month = df$month
df = subset(df, select = -c(id, date))
```

The variable `year` has only two categories, 2014 and 2015, and does not need further processing. However, `month` is a multi-categorical variable, so it is not ideal to use only one regression coefficient to explain the change in relationship between the multi-categorical variables and its influence on the dependent variable. Therefore, we convert `month` into 12 dummy variables representing different months, using "1" for "yes" and "0" for "no". In this way, the results of regression are easier to interpret and have more practical utility, an example would be identifying months that have particularly strong influence on house prices.

```{r}
# dummy (month, True(1), False(0))
for (month_num in 1:12){
  df[paste0("month_", month.abb[month_num])] <- +(df$month == month_num)
}

df = subset(df, select = -(month))
```

## ZIP Code Transformation {#section2b}

For the same reason, the variable `zipcode` is multi-categorical variable with 199 categories, which needs to be converted into dummy variables. Considering that zip codes can be used for positioning, we divided the variable `zipcode` into two categories, one format "980xx" and the other format "981xx", so that it can represent two different regions. "981xx" largely corresponds to areas within Seattle, WA and "980xx" specifies the neighboring suburban areas. From a statistical standpoint, too many dummy variables results in a decrease in degrees of freedom and for small samples might cause number of predictors to exceed the sample size $(p > n)$.

```{r}
# dummy (zipcode, Divided into two groups, 980xx and 981xx)
df$zipcode_start = as.integer(substr(df$zipcode, 1, 3))
df = subset(df, select = -(zipcode))

knitr::kable(
  table(df$zipcode_start), 
  col.names=c("zipcode","frequency"), 
  caption = "ZIP Code Distribution"
) %>%
kableExtra::kable_styling(full_width = FALSE)
```

## Train/Test Split {#section2c}

```{r}
# Split into train/ test set:
set.seed(1023)
sample_ind = sample(nrow(df), round(0.7*nrow(df)))
train <- df[sample_ind, ]
test <- df[-sample_ind, ]

if (nrow(df) == nrow(train) + nrow(test)) {
  print(paste0("Split OK: Train dataset has ", nrow(train), " rows"))
} else {
  rm(train, test)
}

summary(train)
```

## Correlation Analysis {#section2d}

Using the correlation matrix and graph, it can be found that variables `sqft_living`, `grade`, `sqft_above` and `sqft_living15` have a relatively high correlation with the response variable `price`, with a Pearson correlation coefficient around 0.6. However, the variables `sqft_lot`, `condition`, `yr_built`, `long`, `sqft_lot15`, `year`, `zipcode_start` and 12 different months have a very low correlation with `price`, all below 0.1.

```{r}
cor_matrix = round(cor(train), 3)
cor_matrix
```

```{r}
corrplot(cor_matrix, method = "circle")
```

To further examine the top variables correlating with "price", we plot several scatter plots below. This initial analysis shows that price is indeed positively correlated with `sqft_living`, `grade`, `sqft_above` and `sqft_living15`. It also appears that the prices of homes show a large range from 75,000 to 7,700,000, with most data points concentrated on the lower half of this scale. We will perform further testing during model development to understand if a transformation of the price variable would be beneficial.

```{r, fig.width=7, }
par(mfrow=c(2,2))

top.corr<-c("sqft_living", "grade", "sqft_above", "sqft_living15")
for (i in top.corr){
  plot(x=df[,i],y=df[,"price"], main = paste("price vs",i), col= "lightblue",
       xlab=i,ylab="price")
}
```

## Distribution Analysis {#section2e}

Using bar charts and box plots, we can observe the types and distribution of all variables. The results are summarized in the table below.

```{r, fig.width=14, echo = FALSE, fig.height=8}
# Bar charts
par(mfrow=c(2,2), cex = 1.6, mar = c(1.8, 1, 2, 1))

for (i in 1:ncol(df)){
  hist(
    df[,i], 
    main = names(df[i]), 
    xlab = NULL,
    col = "lightblue"
  )
}

```

```{r, fig.width=8, fig.height=8}
# Box plots on continuous variables only
par(mfrow=c(2,2))

categorical.var<-c("bedrooms","bathrooms","floors","waterfront","view","condition","grade","yr_built","yr_renovated","year","zipcode_start",colnames(df)[grep("month",colnames(df))])
df.boxplot<-dplyr::select(df,-all_of(categorical.var))

for (i in 1:ncol(df.boxplot)){
  boxplot(df.boxplot[,i], main = names(df.boxplot[i]), xlab = NULL, col = "lightblue")
}

```

## Summary Table of Data (Post Transformations) {#section2f}

| Variable| Description | Type | Correlation with "price" |
| :-------:| :-------: | :-------: | :-------: |
| price| **Price of each home sold (Response variable)**    | continuous| 1|
| bedrooms| *Number of bedrooms*    | categorical| 0.306|
| bathrooms| *Number of bathrooms, where ".5" accounts for a bathroom with a toilet but no shower*    | categorical| 0.522|
| sqft_living| *Square footage of the apartment interior living space*    | continuous| 0.707|
| sqft_lot| *Square footage of the land space*    | continuous| 0.092|
| floors| *Number of floors*    | categorical| 0.363|
| waterfront| *A dummy variable for whether the apartment was overlooking the waterfront or not*    | categorical| 0.291|
| view| *An index from 0 to 4 of how good the view of the property was*    | categorical| 0.393|
| condition| *An index from 1 to 5 on the condition of the apartment,*    | categorical| 0.050|
| grade| *An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 has a high-quality level of construction and design.*    | categorical| 0.669|
| sqft_above| *The square footage of the interior housing space that is above ground level*    | continuous| 0.607|
| sqft_basement| *The square footage of the interior housing space that is below ground level*    | continuous| 0.335|
| yr_built| *The year the house was initially built*    | categorical| 0.051|
| yr_renovated| *The year of the house's last renovation*    | categorical| 0.129|
| lat| *Latitude*    | continuous| 0.306|
| long| *Longitude*    | continuous| 0.017|
| sqft_living15| *The square footage of interior housing living space for the nearest 15 neighbors*    | continuous| 0.589|
| sqft_lot15| *The square footage of the land lots of the nearest 15 neighbors*    | continuous| 0.081|
| year| *Year of the home sale* | categorical| 0.005|
| month_Jan| *A dummy variable for whether it is January or not* | categorical| -0.007|
| month_Feb | *A dummy variable for whether it is February or not* | categorical| -0.025|
| month_Mar| *A dummy variable for whether it is March or not* | categorical| 0.004|
| month_Apr| *A dummy variable for whether it is April or not* | categorical| 0.019|
| month_May| *A dummy variable for whether it is May or not* | categorical| 0.016|
| month_Jun| *A dummy variable for whether it is June or not* | categorical| 0.017|
| month_Jul| *A dummy variable for whether it is July or not* | categorical| 0.010|
| month_Aug| *A dummy variable for whether it is August or not* | categorical| -0.005|
| month_Sep| *A dummy variable for whether it is September or not* | categorical| -0.017|
| month_Oct| *A dummy variable for whether it is October or not* | categorical| -0.0001|
| month_Nov| *A dummy variable for whether it is November or not* | categorical| -0.013|
| month_Dec| *A dummy variable for whether it is December or not* | categorical| -0.015|
| zipcode_start| *A dummy variable that indicates whether zipcode starts with 980 or 981* | categorical| -0.007|


The barplots and boxplots further indicate that distribution of price is skewed towards the right, with a median of 450,000 and a small subset of houses that are over 2,000,000. Several other factors also show a similar right-skewed distribution, for example "sqft_living" and "sqft_lot". We also notice some data points that may be outliers, for example a house that has 33 bedrooms or some houses that are recorded as having 0 bedroom 0 bathrooms. Further testing will be performed in later sections to evaluate whether variable transformation is needed for model building.


\newpage
# III. Model Development Process (15 points) {#section3}

```{r, include = FALSE}
# Build a regression model to predict price.  And of course, create the train data set which contains 70% of the data and use set.seed (1023). The remaining 30% will be your test data set. Investigate the data and combine the level of categorical variables if needed and drop variables. For example, you can drop id, Latitude, Longitude, etc.
```

## Feature Transformations {#section3a}

```{r}
set.seed(1023)

response <- "price"

#Renaming to conform to unified convention
kc.house.df <- df %>% rename("year_built" = "yr_built")

#Quality Adjusted Features
transform_sqft_adj_grade <- function(kc.house.df){
  sqft_adj_grade <- kc.house.df[, "sqft_living"] * kc.house.df[, "grade"]
  
  return (sqft_adj_grade)
}

transform_sqft_adj_condition <- function(kc.house.df){
  sqft_adj_condition <- kc.house.df[, "sqft_living"] * kc.house.df[, "condition"]
  
  return (sqft_adj_condition)
}

transform_sqft_adj_waterfront <- function(kc.house.df){
  sqft_adj_waterfront <- kc.house.df[, "sqft_living"] * kc.house.df[, "waterfront"]
  
  return (sqft_adj_waterfront)
}

#Polynomial Terms
transform_poly_sqft_living <- function(kc.house.df){
  #Center variables for polynomial terms, to reduce MC
  sqft_living <- (kc.house.df$sqft_living - mean(kc.house.df$sqft_living)) 
  sqft_living_squared <- sqft_living^2
  
  return (list(center = sqft_living, squared = sqft_living_squared))
}

transform_poly_floor <- function(kc.house.df){
  #Center variables for polynomial terms, to reduce MC
  floors <- (kc.house.df$floors - mean(kc.house.df$floors)) 
  floors_squared <- floors^2
  
  return(list(center = floors, squared = floors_squared))
}
```

We create quality adjusted measures because a square foot of property isn't equally valuable across properties. A square foot is more expensive for a high-rise condo (high quality) than it is for a dilapidated home (low quality). Hence the reason we thought to include these features. 

For polynomial terms, we wanted to capture the diminishing marginal utility effect for floors and square footage. Intuitively, adding 300 square foot to a studio apartment increases its value a lot more than adding 300 square foot to a mansion in Atherton. The same can be said about floors.

In terms of interpretation, centering the variables does not change the interpretation of the coefficients. However they do change the interpretation of the intercept. Instead of the intercept being read as the value of price when all variables are zero. It is not read as price when all variables are zero, and when `floors` is equal to its average and `sqft_living` is equal to its average.

```{r}
set.seed(1023)

# Drop December dummy variable due to collinearity with other months
kc.house.df<-dplyr::select(kc.house.df,-(month_Dec))

#Apply transformations 

kc.house.df$sqft_adj_grade <- transform_sqft_adj_grade(kc.house.df)
kc.house.df$sqft_adj_condition <- transform_sqft_adj_condition(kc.house.df)
kc.house.df$sqft_adj_waterfront <- transform_sqft_adj_waterfront(kc.house.df)

res <- transform_poly_sqft_living(kc.house.df)
kc.house.df$sqft_living <- res$center
kc.house.df$sqft_living_squared <- res$squared

res <- transform_poly_floor(kc.house.df)
kc.house.df$floors <- res$center
kc.house.df$floors_squared <- res$squared

#Remove collinear variables
# sqft_basement + sqft_above = sqft_living
kc.house.df$sqft_basement <- NULL
```

We apply the transformations and drop `sqft_basement` due to perfect multicollinearity because `sqft_basement` can be expressed as a linear combination of `sqft_above` and `sqft_living`.

## Train/Test Split {#section3b}

```{r section3d}
set.seed(1023)

#use 70% of dataset as training set and 30% as test set
train_prop <- 0.7

index <- sample(1:nrow(HouseSales), size = round(train_prop * nrow(kc.house.df)))
kc.house.train.X <- kc.house.df[index, -which(names(kc.house.df) %in% c(response))] 
kc.house.train.y <- kc.house.df [index, response]

kc.house.test.X <- kc.house.df [-index, -which(names(kc.house.df) %in% c(response))]
kc.house.test.y <- kc.house.df [-index, response]
```

From the specs, we set the seed via `set.seed(1023)` and create a 70/30 train and test split respectively. We now proceed to fit the model on the training dataset.

```{r}
set.seed(1023)
#baseline model
baseline.model <- lm(price ~ ., data = cbind(price = kc.house.train.y, kc.house.train.X))
summary(baseline.model)
```

From the model output we see that all variables are significant except for `sqft_above`, `zipcode_start`, `grade`, `floor_squared` and month dummies May - Nov. The adjusted $R^2$ is 0.7463. Interestingly the quality-adjusted and polynomial features we created in this section are very significant, which provide evidence that there exists diminishing marginal returns and the need for quality adjustment. The p-value for `floors_squared` is just under an $\alpha = 0.05$ most likely because most homes don't have high of floors to the point where diminishing returns start to effect. Regardless we see the effect a bit in the regression.

Lastly not as important since the project focuses on prediction but including dummies for `month` and `year` can be thought of as month and year fixed effects, which means our model coefficients are robust to bias from unobservable confounding on the year and month level, assuming the unobservables are time-invariant. Essentially, we remove variation between months and years. For example if an unobservable like rainfall has an effect on price it will controlled via the fixed effects assuming rainfall in March is the same across time. An example for years, would be proportion of white people because demographics change extremely slowly and in our short time horizon can be assumed to be constant over years. The effect of proportion of white people on price would be controlled for by the year fixed effect.

## Model Diagnostic Plots {#section3c}

We now proceed to provide diagnostic plots as a preview to any problems that will be remedied. The first four plots are the staples.

`Residuals vs Fitted`: The residuals look centered around zero as indicated by the red line, however residuals are hetereoskedastic with a megaphone shape to it. This suggests a need for weighted least squares as remedy.

`QQ Plot`: The residuals look very non-normal as the points deviate heavily from the expected values under normality. The shape indicate that the residuals have heavy tails.

`Scale Location`: The red line is increasing indicating presence of hetereoskedasticity, specifically the variance of the residuals is increasing with fitted values. This plot agrees with the `Residuals vs Fitted` that the non-constant variance assumption is violated.

`Residuals vs Leverage`: There are two observations that are above 1, indicating that the points are very likely to be influential. Cook's distance summarizes the effect of case $i$ on all fitted values, meaning observations 7253 and 8093 are likely to influence the baseline regression coefficients quite a bit.

Violations of constant variance assumption and normality affect one's ability to conduct inference on the model (e.g. hypothesis testing on coefficients) because those test statistics (e.g. t-test) assume normality. Hetereoskedasticity affects the standard errors of inferential tests and coefficients causing them to be unreliable and less precise. Recall that the Gauss Markov theorem stated that the variance of the coefficients are the most efficient but only when the homokedasticity assumption is upheld. Lastly hetereoskedasticity does not bias a model's coefficients. It only affects the standard errors.

```{r, fig.width=8, fig.height=8}
par(mfrow=c(2,2))
plot(baseline.model)
```

```{r}
par(mfrow=c(2,2))
#Residual vs Leverage
ols_plot_resid_lev(baseline.model)

#Cook's Distance Visualized
ols_plot_cooksd_chart(baseline.model)

#DFFITS
ols_plot_dffits(baseline.model)
```

From the `Outlier and Leverage Diagnostics` plot we see observations 13244, 643, 11395, and 9986 as one of the many possibly problematic points, as they are labeled as "outlier & leverage". Moving on the `Cook's Distance` plot, which measures case $i$ influence on all fitted values, observations 13244 and 643 are standouts. Lastly we move on to `DFFITS`, which measures influence of case $i$ on fitted value $i$ $(\hat{y}_i)$, again 13244 and 643 are standout points.

```{r}
#DFBETAS, plotting only two predictors for brevity sake
plot_ob = ols_plot_dfbetas(baseline.model, print_plot = FALSE)
plot_ob$plots[4]
plot_ob$plots[10]
```

Finally for completeness we include `DFBETAS` and for brevity sake only included two predictors but curious readers can adjust code if need be. The plots show the same observations flagged in prior metrics. For `sqft_living` observations 643 and 13244 greatly influence the value of the coefficient on `sqft_living`. The same two points also have strong influence on the coefficient on `grade`.

## Model Diagnostics Statistical Tests {#section3d}

To support all statements prior, we will not conduct statistical tests to quantitatively support or deny the visual plots in the previous subsection. We will use the Anderson-Darling test to test for normality because when Shapiro-Wilk test was applied R threw and error, saying the sample size is too large for Shapiro-Wilk and suggest AD test instead. The hypotheses for the test are as follows.

$$
H_0: \text{Data is normally distributed}
$$

$$
H_1: \text{Data is not normally distributed}
$$

```{r}
ad.test(baseline.model$residuals)
```

The results of the Anderson-Darling test confirm the visual results in the prior subsection. The p-value is small, much less than the standard $\alpha = 0.05$, so we reject the null and conclude that residuals for the baseline OLS model is not normally distributed.

Next we apply Breusch Pagan test to statistically test the constant variance assumption for residuals. The hypotheses for the test are as follows.

$$
H_0: \text{Error Variance are constant}
$$

$$
H_1: \text{Error Variance are not constant}
$$

```{r}
#test statistic is chi square distributed
bptest(baseline.model, studentize=FALSE)
```

As expected, the results of the Breusch Pagan test confirm the visual results in the prior subsection. The p-value is small, much less than the standard $\alpha = 0.05$, so we reject the null and conclude that error variance is not constant.

\newpage

# IV. Model Performance Testing (15 points) {#section4}

```{r, include = FALSE}
# *Use the test data set to assess the model performances. Here, build the best multiple linear models by using the stepwise both ways selection method. Compare the performance of the best two linear models. Make sure that model assumption(s) are checked for the final linear model. Apply remedy measures (transformation, etc.) that helps satisfy the assumptions. In particular you must deeply investigate unequal variances and multicollinearity. If necessary, apply remedial methods (WLS, Ridge, Elastic Net, Lasso, etc.). *
```

## Overview {#section4a}

In this section we will try several modeling approaches and evaluate each on the test set. The model to be tested are 1) baseline 2) stepwise both ways 3) Box-Cox transformed OLS 4) ridge 5) lasso 6) elastic net 7) elastic net normalized 6) robust regression with huber weights.


## Baseline OLS {#section4b}

First, we evaluate the baseline model's performance on the test set.

```{r}
set.seed(1023)
#Function to generate model performance
CalcTestMetrics <- function(pred, act, n, p) {
  SST <- var(act)*(length(act)-1)
  SSE <- sum((act-pred)^2)
  SSR <- sum((pred - mean(act))^2)
  rsquared <- 1- SSE/SST
  
  adj.rsquared <- 1 - (((1 - rsquared)*(n-1)) / (n-p-1))
  mse <- sum((act - pred)^2) / (n-p)
  mae <- (sum(abs(act-pred))) / n
  
  c(adj.rsquared = adj.rsquared, 
    rsquared = rsquared, 
    mse = mse, 
    mae = mae)
}

# Create coefficients table and join model coefficients
coeff.lm.df <- data.frame(Baseline = baseline.model$coefficients)
coeff.lm.df$Coefficients <- rownames(coeff.lm.df)
rownames(coeff.lm.df) <- NULL
coeff.lm.df <- dplyr::select(coeff.lm.df, Coefficients, Baseline) 
coeff.q1.df <-
  coeff.lm.df %>%
  dplyr::mutate(Baseline = ifelse(Baseline == 0, "--", scales::comma(Baseline, accuracy = 1e-4)))
```


```{r}
set.seed(1023)
# Using the function to calculate performance of the baseline model on the test set
pred <- predict(baseline.model, kc.house.test.X)
act <- kc.house.test.y
n <- dim(model.matrix(baseline.model))[1]
p <- dim(model.matrix(baseline.model))[2]

performance_baseline = CalcTestMetrics(pred, act, n, p)
performance_baseline
```

The adjusted $R^2$ on the test set for the baseline OLS model is 0.751.

## Stepwise Both Ways {#section4c}

Next, we applied stepwise variable selection in both directions using p-value as the selection criteria.

```{r part IV - Stepwise both ways}
set.seed(1023)
#Parameters pent=0.35, prem=0.05 ensure final model's predictors are significant
stepwise_model = ols_step_both_p(baseline.model, pent=0.35, prem=0.05)

ggplot(
  data.frame(adjr = stepwise_model$adjr, index = 1:length(stepwise_model$adjr)), 
  aes(x = index, y = adjr)
) +
ylim(0.71, 0.78) + 
geom_hline(yintercept=0.755, linetype="dashed", color = "red") + 
annotate("text", x=6, y=0.758, label="0.755", size = 3, color = "red") +
geom_point(size = 4, color='darkturquoise') +
labs(x = "Step Index", y = "Adjusted R-squared", title = "Adjusted R-squared (Stepwise Both Ways)")
```

```{r}
set.seed(1023)
stepwise_final = stepwise_model$model
summary(stepwise_final)
```

Stepwise both ways reduced the set of predictors and all predictors are significant as intended. Next I will identify which variables were selected and create a new train/test dataset to run the stepwise OLS model.

```{r part IV -Identifying selected and eliminated variables}
set.seed(1023)
#Generating new dataframes with only the selected variables
selected_variables = names(coef(stepwise_final))
selected_variables = setdiff(selected_variables, "(Intercept)")
selected_train_df = kc.house.train.X[, selected_variables]
selected_test_df = kc.house.test.X[, selected_variables]

#Finding out which ones were dropped
original_predictors = names(kc.house.train.X)
dropped_variables = setdiff(original_predictors, selected_variables)

dropped_variables
```

Stepwise selection dropped twelve variables from our original set of thirty-one predictors. Then were month dummies and the others were `sqft_lot`, `sqft_above` and `grade`. Now we checked for the presence of multicollinearity using VIF.

```{r}
set.seed(1023)
#Fitting a new linear model with selected variables
selected_linear_model = lm(price ~ . , data = cbind(price = kc.house.train.y, selected_train_df))

# Coefficients
coeff.sw.df <- data.frame(Stepwise = selected_linear_model$coefficients)
coeff.sw.df$Coefficients <- rownames(coeff.sw.df)
coeff.q1.df <-
  coeff.q1.df %>%
  dplyr::left_join(coeff.sw.df, by = "Coefficients") %>%
  dplyr::mutate(Stepwise = ifelse(dplyr::coalesce(Stepwise, 0) == 0, "--", 
                                  scales::comma(Stepwise, accuracy = 1e-4)))

#Checking performance on the test set
pred_selected <- predict(selected_linear_model, selected_test_df)
act_selected <- kc.house.test.y
n_selected <- dim(model.matrix(selected_linear_model))[1]
p_selected <- dim(model.matrix(selected_linear_model))[2]

performance_selected = CalcTestMetrics(pred_selected, act_selected, n_selected, p_selected)
performance_selected
```

```{r}
vif_values = vif(selected_linear_model)
variable_names <- names(vif_values)
vif_values <- as.numeric(vif_values)

# Create a data frame with variable names and VIF values
vif_results <- data.frame(Variable = variable_names, VIF = vif_values)
vif_results[vif_results$VIF > 10, ]
```

We have three variables above our cutoff of 10: `sqft_living`, `sqft_adj_grade`, and `sqft_adj_condition`. VIF provides evidence of multicollinearity but it is unknown which specific variables are highly correlated with the four. This will need further investigation. Recall that multicollinearity affects the standard error of the model one way to see its effect is to take the square root.

```{r}
sqrt(vif_results[vif_results$VIF > 10, ]$VIF)
```

This means that the standard error on `sqft_living` is 8 times larger than had multicollinearity not been present.

```{r fig.width=8, fig.height=8}
# Model diagnostic plots
par(mfrow=c(2,2))
plot(selected_linear_model)
```

The diagnostic plots for the stepwise selected model indicates issues with normality assumption as indicated by the `QQ Plot`. The residuals deviate greatly from the expected values under normality and shows a pattern akin to a distribution with heavy tails. The constant variance assumption is also shown to be violated via the `Residuals vs Fitted` plot and the `Scale-Location` plot. These look similar to the baseline OLS model's. The residuals show a megaphone shape and the variance is increasing with fitted values as shown by the increasing red line in the `Scale-Location` plot. Lastly two observations have large Cook's distances which mean influential points may also a problem. 

Variable selection methods don't do well to remedy these particular issues. If we get lucky and predictors that have outliers get dropped or if hetereoskedasticity is caused by one predictor then perhaps it can solve the issue. However in most cases problematic points are scattered across the feature space.

## Box-Cox Transformation {#section4d}

```{r part IV - Applying transformantions, fig.width=8}
set.seed(1023)
library(MASS)
boxcox(selected_linear_model,lambda=seq(-0.2,0.2,0.01))
transformed_model = lm(log(price) ~ . , data = cbind(price = kc.house.train.y, selected_train_df))
summary(transformed_model)

# Model diagnostic
par(mfrow=c(2,2))
plot(transformed_model)
```

Box-cox value indicates optimal lambda at a value very close to zero. Zero also falls within a 95% confidence interval that is very small, suggesting large confidence in a log transformation. After transforming the model (log of the price), the assumptions seem to be fine. To support the visual claims we conduct statistical tests.

The first test is the Anderson-Darling test because as explained in prior sections, the Shapiro-Wilk test does not work on large samples. Refer to [section](#section3d) for the hypotheses of the test.

```{r part IV - Normality tests}
std_residuals = rstandard(transformed_model)
ad.test(std_residuals)
```

The p-value of the test remains small. We reject the null and conclude that residuals are not normal for the transformed model. Next we apply the Lilliefors test which is just a special case of the KS-Test that tests for normality instead of any arbitrary distribution.

```{r}
lillie.test(std_residuals)
```
Lastly we check the homoskedasticity assumption with Breush Pagan test. Refer to [section 3](#section3d) for hypotheses.

```{r}
#test statistic is chi square distributed
bptest(transformed_model, studentize=FALSE)
```

The p-value is small therefore there is still evidence of hetereoskedasticity.

Overall it seems like neither normality nor homoskedasticity holds, even though the Q-Q plot looks much better after transformation. Looks like the standardized residuals are heavy on the tails, and statistical tests shows that normality does not hold as well. It's up to question if this is a huge problem because violations of normality primary affect inference and not prediction. However, non-parametric models should be tried for better results, since these modeling approaches do not need those assumptions.

```{r part IV - Performance of transformed model}
set.seed(1023)
y_test = log(kc.house.test.y)

# Coefficients
coeff.tr.df <- data.frame(StepwiseBoxCox = transformed_model$coefficients)
coeff.tr.df$Coefficients <- rownames(coeff.tr.df)
coeff.q1.df <-
  coeff.q1.df %>%
  dplyr::left_join(coeff.tr.df, by = "Coefficients") %>%
  dplyr::mutate(StepwiseBoxCox = ifelse(dplyr::coalesce(StepwiseBoxCox, 0) == 0, "--", 
                                  scales::comma(StepwiseBoxCox, accuracy = 1e-4)))
knitr::kable(coeff.q1.df, caption = "Model Coefficients")

pred_transformed = predict(transformed_model, selected_test_df)
act_transformed <- y_test
n_transformed <- dim(model.matrix(transformed_model))[1]
p_transformed <- dim(model.matrix(transformed_model))[2]

performance_transformed = CalcTestMetrics(pred_transformed, act_transformed, n_transformed, p_transformed)
print(performance_transformed)
```

Comparing models across coefficients, we see the signs agree except for `waterfront` and `condition`. The stepwise Box-Cox model's coefficients for this set makes more sense because having a waterfront, a good condition home and more square footage should increase a home's price and not decrease it. 

\newpage

## Ridge Regression

Next we attempt to use ridge, lasso and elastic net as techniques to remedy the OLS assumption violations seen in previous parts. We start with ridge regression first. 

```{r problem IV - Ridge regression}
set.seed(1023)
x = data.matrix(selected_train_df)
y = log(kc.house.train.y)

ridge_model = cv.glmnet(x,y,alpha=0, nlambda=100,lambda.min.ratio=0.0001)
best_lambda_ridge = ridge_model$lambda.min

# Coefficients Best Lambda
coeff.ridge.df <- stats::coef(ridge_model , s = best_lambda_ridge)
coeff.ridge.df <- data.frame(Coefficients = coeff.ridge.df@Dimnames[[1]],
                             Ridge = coeff.ridge.df@x)

coeff.df.regularized <- data.frame(Baseline = baseline.model$coefficients)
coeff.df.regularized$Coefficients <- rownames(coeff.df.regularized)
rownames(coeff.df.regularized) <- NULL
coeff.df.regularized <- dplyr::select(coeff.df.regularized, Coefficients, Baseline) 

coeff.df.regularized <-
  coeff.df.regularized %>%
  dplyr::left_join(coeff.ridge.df, by = "Coefficients") %>%
  dplyr::mutate(Ridge = ifelse(dplyr::coalesce(Ridge, 0) == 0, "--", 
                               scales::comma(Ridge, accuracy = 1e-4)))
```

```{r}
#Performance
pred_ridge = predict(ridge_model, s = best_lambda_ridge, 
                 newx = data.matrix(selected_test_df))[,1]
act_ridge <- y_test
n_ridge <- dim(model.matrix(transformed_model))[1]
p_ridge <- dim(model.matrix(transformed_model))[2]

performance_ridge = CalcTestMetrics(pred_ridge, act_ridge, n_ridge, p_ridge)
print(performance_ridge)
```

Ridge regression doesn't move the needle much in terms of predictive performance. Note that ridge does not perform variable selection. The variables that are missing are ones that were selected out from the stepwise step in this subsection. We will examine coefficients once all regularization models are fitted.

## Lasso Regression

```{r problem IV - Lasso regression}
set.seed(1023)
lasso_model = cv.glmnet(x,y,alpha=1, nlambda=100,lambda.min.ratio=0.0001)
best_lambda_lasso = lasso_model$lambda.min

# Coefficients
coeff.lasso <- stats::coef(lasso_model, s = "lambda.min")
coeff.lasso.names <- coeff.lasso@Dimnames[[1]]
coeff.lasso.df <- data.frame()
for (i in 1:length(coeff.lasso)) {
  df <- data.frame(Coefficients = coeff.lasso.names[i],
                   Lasso = coeff.lasso[i])
  coeff.lasso.df <- rbind(df, coeff.lasso.df)
}

coeff.df.regularized  <-
  coeff.df.regularized  %>%
  dplyr::left_join(coeff.lasso.df, by = "Coefficients") %>%
  dplyr::mutate(Lasso = ifelse(dplyr::coalesce(Lasso, 0) == 0, "--", 
                               scales::comma(Lasso, accuracy = 1e-4)))

#Performance
pred_lasso = predict(lasso_model, s = best_lambda_lasso, 
                 newx = data.matrix(selected_test_df))[,1]
act_lasso <- y_test
n_lasso <- dim(model.matrix(transformed_model))[1]
p_lasso <- dim(model.matrix(transformed_model))[2]

performance_lasso = CalcTestMetrics(pred_lasso, act_lasso, n_lasso, p_lasso)
print(performance_lasso)
```

Unfortunately we've come up short once again. There was no predictive performance improvement. Commentary on coefficients will come later.

## Elastic Net

The last set of regularized models we try will be elastic net, which is a blend of lasso and ridge. It is a technique that still performs variable selection can be thought of as a less harsh lasso.

```{r problem IV - Elastic net regression}
set.seed(1023)
enet_model = cv.glmnet(x,y,alpha=0.5, nlambda=100,lambda.min.ratio=0.0001)
best_lambda_enet = enet_model$lambda.min

# Coefficients
coeff.en <- stats::coef(enet_model, s = best_lambda_enet)
coeff.en.names <- coeff.en@Dimnames[[1]]
coeff.en.df <- data.frame()
for (i in 1:length(coeff.en)) {
  df <- data.frame(Coefficients = coeff.en.names[i],
                   ElasticNet = coeff.en[i])
  coeff.en.df <- rbind(df, coeff.en.df)
}
coeff.df.regularized <- 
  coeff.df.regularized %>%
  dplyr::left_join(coeff.en.df, by = "Coefficients") %>%
  dplyr::mutate(ElasticNet = ifelse(dplyr::coalesce(ElasticNet, 0) == 0, "--", 
                                    scales::comma(ElasticNet, accuracy = 1e-4)))

#Performance
pred_enet = predict(enet_model, s = best_lambda_enet, 
                 newx = data.matrix(selected_test_df))[,1]
act_enet <- y_test
n_enet <- dim(model.matrix(transformed_model))[1]
p_enet <- dim(model.matrix(transformed_model))[2]

performance_enet = CalcTestMetrics(pred_enet, act_enet, n_enet, p_enet)
print(performance_enet)

```

There is a small predictive performance from elastic net. We wrap up this section with elastic net regression without log transformation of price is included as another validation and comparison.

```{r problem IV - Elastic net regression without transformation}
set.seed(1023)
x1= data.matrix(selected_train_df)
y1= kc.house.train.y

enet_model1 = cv.glmnet(x1,y1,alpha=0.5, nlambda=100,lambda.min.ratio=0.0001)
best_lambda_enet = enet_model$lambda.min

# Coefficients
coeff.en <- stats::coef(enet_model1, s = best_lambda_enet)
coeff.en.names <- coeff.en@Dimnames[[1]]
coeff.en.df <- data.frame()
for (i in 1:length(coeff.en)) {
  df <- data.frame(Coefficients = coeff.en.names[i],
                   ElasticNetNoTransform = coeff.en[i])
  coeff.en.df <- rbind(df, coeff.en.df)
}
coeff.df.regularized <- 
  coeff.df.regularized %>%
  dplyr::left_join(coeff.en.df, by = "Coefficients") %>%
  dplyr::mutate(ElasticNetNoTransform = ifelse(dplyr::coalesce(ElasticNetNoTransform, 0) == 0, "--", 
                                    scales::comma(ElasticNetNoTransform, accuracy = 1e-4)))
knitr::kable(coeff.df.regularized, caption = "Model Coefficients")

#Performance

y_test1 = kc.house.test.y

pred_enet = predict(
  enet_model1, 
  s = best_lambda_enet, 
  newx = data.matrix(selected_test_df)
)[,1]
act_enet <- y_test1
n_enet <- dim(model.matrix(transformed_model))[1]
p_enet <- dim(model.matrix(transformed_model))[2]

performance_enet2 = CalcTestMetrics(pred_enet, act_enet, n_enet, p_enet)
print(performance_enet2)
```

Comparing across the coefficients the first thing to note is lasso doesn't perform any additional variable selection on top of the set of variables selected by stepwise. We could have opted for a larger lambda but the optimal lambda seems to be pretty lax. All regularized models have the same set of predictors.

In terms of coefficient direction, there are a few interesting ones to point out. The first is `sqft_lot15`. The baseline model and the untransformed elastic net, suggest that a home surrounded by homes that are large decreases price. Perhaps we are detecting an effect where a home's price is relative to peers. In other words it's better to be a big fish in a small pond. Other coefficients' signs make sense like `yr_renovated` is positive because renovation boosts a home price and `year_built` because newer homes are more valuable.

## Robust Regression

Finally, we included robust regression due to our observation of potential outliers. We will use huber weights.

```{r problem IV - Robust regression}
set.seed(1023)
robust_model = rlm(log(price) ~ . , data = cbind(price = kc.house.train.y, selected_train_df), psi = psi.huber)
summary(robust_model)


coeff.df.robust <- data.frame(Baseline = baseline.model$coefficients)
coeff.df.robust$Coefficients <- rownames(coeff.df.robust)
rownames(coeff.df.robust) <- NULL
coeff.df.robust <- dplyr::select(coeff.df.robust, Coefficients, Baseline) 

# Coefficients
coeff.huber.df <- data.frame(Robust = robust_model$coefficients)
coeff.huber.df$Coefficients <- rownames(coeff.huber.df)
coeff.df.robust <-
  coeff.df.robust %>%
  dplyr::left_join(coeff.huber.df, by = "Coefficients") %>%
  dplyr::left_join(coeff.df.regularized[, c("ElasticNet", "Coefficients")], by = "Coefficients") %>%
  dplyr::mutate(Robust = ifelse(dplyr::coalesce(Robust, 0) == 0, "--",
                               scales::comma(Robust, accuracy = 1e-4)))
knitr::kable(coeff.df.robust, caption = "Model Coefficients")

#Performance
pred_robust = predict(robust_model, selected_test_df)
act_robust <- y_test
n_robust <- dim(model.matrix(robust_model))[1]
p_robust <- dim(model.matrix(robust_model))[2]

performance_robust = CalcTestMetrics(pred_robust, act_robust, n_robust, p_robust)
print(performance_robust)

```

The robust regression coefficient align with the other models. Again there was no predictive lift. As a final step we will check the huber weights to see how well robust regression dealt with outliers and leverage points.

```{r}
set.seed(1023)
huber.weights <- data.frame(Obs = c(1:nrow(selected_train_df)), resid = robust_model$resid, weight = robust_model$w)
huber.weights <- huber.weights[order(robust_model$w), ]

#Preview the data
huber.weights[1:15, ]
```
Interestingly the problem observations identified in previous parts (e.g 13244, 643, 11395, and 9986) did not appear in the lowest 15 weighted observations. Perhaps Huber wasn't able to accurately suppress the problematic points or they were selected out via the stepwise model.

## Summary

```{r problem IV - R2 table}
library(knitr)
library(dplyr)

results_df <- data.frame(
  Model = c('Baseline', 'Stepwise', 'Transformed', 'Ridge', 'Lasso', 'Elastic Net', 'Robust', "Elatic Net (Untransformed)"),
  Adj.RSquared = c(
      performance_baseline["adj.rsquared"], 
      performance_selected["adj.rsquared"],
      performance_transformed["adj.rsquared"],
      performance_ridge["adj.rsquared"],
      performance_lasso["adj.rsquared"],
      performance_enet["adj.rsquared"],
      performance_robust["adj.rsquared"],
      performance_enet2["adj.rsquared"]
  ),
  RSquared = c(
      performance_baseline["rsquared"], 
      performance_selected["rsquared"],
      performance_transformed["rsquared"],
      performance_ridge["rsquared"],
      performance_lasso["rsquared"],
      performance_enet["rsquared"],
      performance_robust["rsquared"],
      performance_enet2["rsquared"]
  ), 
  MSE = c(
      performance_baseline["mse"], 
      performance_selected["mse"], 
      performance_transformed["mse"],
      performance_ridge["mse"],
      performance_lasso["mse"],
      performance_enet["mse"],
      performance_robust["mse"],
      performance_enet2["mse"]
  ), 
  MAE = c(
      performance_baseline["mae"], 
      performance_selected["mae"], 
      performance_transformed["mae"],
      performance_ridge["mae"],
      performance_lasso["mae"],
      performance_enet["mae"],
      performance_robust["mae"],
      performance_enet2["mae"]
  ) 
)

# Sort and display the table
sorted_results_df <- dplyr::arrange(results_df, desc(Adj.RSquared))
knitr::kable(sorted_results_df, caption = "Model Metrics", format = "markdown")
```

Overall, Elastic Net model is the best performing model as it has the highest adjusted R-squared value and the lowest MSE (Note that baseline and stepwise models did not undergo log transformation of the dependent variable so their MSE values cannot be directly compared to that of the other log-transformed models).

```{r include=FALSE}
all_models <-
  coeff.q1.df %>%
  dplyr::left_join(coeff.df.regularized[, -2], by = "Coefficients") %>% #drop extra Baseline column
  dplyr::left_join(coeff.df.robust[, c("Robust", "Coefficients")], by = "Coefficients")
all_models
```

\newpage
# V. Challenger Models (15 points) {#section5}

*Build an alternative model based on one of the following approaches to predict price: regression tree, NN, or SVM. Explore using a logistic regression. Check the applicable model assumptions. Apply in-sample and out-of-sample testing, backtesting and review the comparative goodness of fit of the candidate models. Describe step by step your procedure to get to the best model and why you believe it is fit for purpose.*

Part V

## Overview

We first built a basic regression tree model as a baseline non-parametric challenger model; results were not particularly strong (r-squared at ~0.65, which is lower than the r-squared of the linear regression model). We then tuned hyperparameters for the decision tree using random search (using RMSE and MAE as the metrics to optimise) and backtested the model using a cross-validation resampling approach. This did not achieve significant improvement in performance (r-squared at ~0.60). Ultimately we chose this regression tree model as our challenger model, as it is relatively simple to build and implement, and fairly explainable (e.g. the splits tend to be easy to understand). 

Analysis of residuals showed that while there was significant skewness on the left and right tails, the normality assumption broadly holds; and a 2 sided t test confirmed that there is no significant difference between actual and predicted values. More complex models e.g. deep neural networks would likely perform better, but would likely consume too many computational resources for our purposes.


## Building baseline regression tree model
```{r Part V, fig.width=7}
# Load libraries
library(caret)
library(rpart)
library(nnet)
library(knitr)
library(dplyr)
library(stringr)
library(readr)
library(kableExtra)
library(mlr)
library(rpart.plot)

# Model building

# Build basic regression tree
tree_model <- rpart(price ~ ., data = train)

# Visualize the decision tree 
rpart.plot(tree_model, shadow.col = "gray", nn = TRUE)

# Visualize important features:
feature.imp<-data.frame(importance=tree_model$variable.importance)

feature.imp %>%
  tibble::rownames_to_column(var = "feature") %>%
   ggplot(aes(x = forcats::fct_reorder(feature, importance), y = importance)) +
   geom_pointrange(aes(ymin = 0, ymax = importance), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "Importance",title="Importance of model features")
```

```{r}
# Make predictions using basic regression tree
tree_predictions <- predict(tree_model, newdata = test)
tree_rmse <- sqrt(mean((tree_predictions - test$price)^2))
tree_mae <- mean(abs(tree_predictions - test$price))
tree_mse <- mean((tree_predictions - test$price)^2)

#View basic regression tree results
cat("Regression Tree RMSE:", tree_rmse, "\n")
cat("Regression Tree MAE:", tree_mae, "\n")
cat("Regression Tree MSE:", tree_mse, "\n")

```

## Hyperparameter tuning for regression tree 
```{r}
#Tune regression tree hyperparameters to improve performance

# Define the hyperparameter space for random search
param_space <- makeParamSet(
  makeNumericParam("cp", lower = 0.01, upper = 0.1),
  makeIntegerParam("minsplit", lower = 5, upper = 10),
  makeIntegerParam("minbucket", lower = 5, upper = 10),
  makeIntegerParam("maxdepth", lower = 5, upper = 15),
  makeIntegerParam("maxcompete", lower = 0, upper = 1)
)

# Create a learner for regression with rpart
learner <- makeLearner("regr.rpart", predict.type = "response")

# Define the backtesting strategy (in this case, 10-fold cross-validation)
resampling <- makeResampleDesc("CV", iters = 10)

# Define the random search control object
ctrl <- makeTuneControlRandom(maxit = 10)

# Perform random search for hyperparameter tuning
random_search <- tuneParams(
  learner = learner,
  task = makeRegrTask(data = train, target = "price"),
  resampling = resampling,
  par.set = param_space,
  control = ctrl,
  measures = list(rmse, mae)  # Specify evaluation metrics
)

# Get the best hyperparameters
best_hyperparameters <- random_search$x

# Print the best-tuned hyperparameters
print(best_hyperparameters)

# Train the final model using the best hyperparameters
tree_model_tuned <- rpart(
  price ~ .,
  data = train,
  control = rpart.control(
    cp = best_hyperparameters$cp,
    minsplit = best_hyperparameters$minsplit,
    minbucket = best_hyperparameters$minbucket,
    maxdepth = best_hyperparameters$maxdepth,
    maxcompete = best_hyperparameters$maxcompete
  )
)

# Make predictions using the final tuned model
predictions <- predict(tree_model_tuned, newdata = test)
```

## Diagnostic plots and performance comparisons for regression tree model
```{r}
# Calculate residuals
residuals <- test$price - predictions
# Create diagnostic plots using
par(mfrow = c(2, 2))  

# Scatter plot of residuals vs. predictions
ggplot(data = data.frame(Predictions = predictions, Residuals = residuals)) +
  geom_point(aes(x = Predictions, y = Residuals)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Predictions",
       x = "Predictions",
       y = "Residuals") +
  theme_minimal()

# Histogram of residuals
ggplot(data = data.frame(Residuals = residuals)) +
  geom_histogram(aes(x = Residuals), bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency") +
  theme_minimal()

# Normal Q-Q plot of residuals
qqplot_data <- data.frame(Standardized_Residuals = scale(residuals))
ggplot(qqplot_data, aes(sample = Standardized_Residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Normal Q-Q Plot of Residuals") +
  theme_minimal()
```

The above plots show that residuals tend to increase as prediction values increase. This is less of an issue for non-parametric models like the regression tree, but it does indicate that model accuracy may vary over different range of values. We could further investigate if this leads to significant concerns related to model performance. The histogram and normal Q-Q plot for residuals indicate that they are mostly normally distributed, although there are long tails in both the left and right directions. 

```{r}
# Residuals vs. Index plot
ggplot(data = data.frame(Residuals = residuals, Index = seq_along(residuals))) +
  geom_point(aes(x = Index, y = Residuals)) +
  labs(title = "Residuals vs. Index",
       x = "Index",
       y = "Residuals") +
  theme_minimal()

# Paired t-test for differences between observed and actual values
t_test <- t.test(test$price, tree_predictions, paired = TRUE)
cat("\nPaired t-test for Differences:\n")
cat("t statistic:", t_test$statistic, "\n")
cat("p-value:", t_test$p.value, "\n")
if (t_test$p.value < 0.05) {
  cat("There is a significant difference between observed and predicted values (reject null hypothesis).\n")
} else {
  cat("There is no significant difference between observed and predicted values (fail to reject null hypothesis).\n")
}

# Compare model performance
compare_model_performance <- function(models, model_names, train, test) {
  rmse_values <- c()
  rsquared_values <- c()
  mape_values <- c()
 
  for (i in 1:length(models)) {
    model <- models[[i]]
    predictions <- predict(model, newdata = test)
    rmse <- sqrt(mean((test$price - predictions)^2))
    rsquared <- cor(test$price, predictions)^2
    mape <- mean(abs((test$price - predictions) / test$price)) * 100
   
    rmse_values <- c(rmse_values, rmse)
    rsquared_values <- c(rsquared_values, rsquared)
    mape_values <- c(mape_values, mape)
  }
 
  return(list(rmse_values, rsquared_values, mape_values))
}

# Call the function to compare model performance
models <- list(tree_model_tuned, tree_model)  
model_names <- c("Tuned regression tree model", "Basic regression tree model")  
performance_metrics <- compare_model_performance(models, model_names, train, test)

# Create a data frame to hold the results
model_results <- data.frame(Model = model_names,
                             RMSE = performance_metrics[[1]],
                             R_squared = performance_metrics[[2]],
                             MAPE = performance_metrics[[3]])

# Create a table to compare model results
kable(model_results, format = "markdown")
```

Overall, the regression tree models achieve pseudo R squared values of 0.60-0.66. A 2 sided t-test confirmed that there is no significant difference between actual and predicted values. This leads us to believe that the regression trees are reasonable models for predicting house prices in this data set, but they still under-perform compared to some of the linear models shown in previous sections.

\newpage
# VI. Model Limitation and Assumptions (15 points) {#section6}

## Overview

In section IV, after log transforming the price variable the Q-Q plot was much smoother, indicating that residuals are more akin to normal than before. There was little difference in performance between the robust regressions, and an elastic net model was selected as a primary model. We believe the elastic net will serve as a conservative predictor due to its mix of L1 and L2 regularization and should handle complex new data well.

We have selected a regression tree as the benchmark model because we believe that it is well suited to predict continuous data with a variety of predictor variables. In addition, as a non-parametric model it has the benefit of being resistant to the problems such as non-normality and heteroskedacity as seen in earlier sections. However, regression trees are often prone to over-fitting, which is why we will be comparing the predictions and outputs of this model against the elastic net model as the elastic net is more regularized. RMSE will be the best indicator for prediction error, as it is better for interpreting and a common metric for prediction.

## Performance comparison between elastic net and regression tree models

```{r}
#Actual and predicted values for the two models
tree.actual = test$price
enet.actual = y_test # this is log-transformed
tree.predicted = predict(tree_model_tuned, newdata = test)
enet.predicted = predict(enet_model, s = best_lambda_enet,
                 newx = data.matrix(selected_test_df))[,1]
 
#Summarize R2 and error statistics for tree regression
tree.sse <- sum((tree.actual - tree.predicted)^2)
tree.rmse <- sqrt(mean((tree.actual - tree.predicted)^2))
tree.mae <- mean(abs(tree.actual - tree.predicted))
tree.sst <- sum((tree.actual - mean(tree.actual))^2)
tree.pseudo_r_squared <- 1 - (tree.sse / tree.sst)

#Summarize R2 statistics for elastic net on enet predictions (based on log-transformed price)
enet.sse <- sum((enet.actual - enet.predicted)^2)
#enet.rmse <- sqrt(mean((enet.actual - enet.predicted)^2))
#enet.mae <- mean(abs(enet.actual - enet.predicted))
enet.sst <- sum((enet.actual - mean(enet.actual))^2)
enet.pseudo_r_squared <- 1 - (enet.sse / enet.sst)

# Summarize error statistics for enet predictions (based on "un-logged" price)
non_transformed_predictions <- exp(enet.predicted)
enet.actual.exp = y_test1
enet.sse.exp <- sum((enet.actual.exp - non_transformed_predictions)^2)
enet.rmse.exp <- sqrt(mean((enet.actual.exp - non_transformed_predictions)^2))
enet.mae.exp <- mean(abs(enet.actual.exp - non_transformed_predictions))
enet.sst.exp <- sum((enet.actual.exp - mean(enet.actual))^2)


model.results <- data.frame(
  Metric = c("Pseudo R-squared", "SSE", "RMSE", "MAE"),
  Regression_Tree = c(tree.pseudo_r_squared, tree.sse, tree.rmse, tree.mae),
  Elastic_Net = c(enet.pseudo_r_squared, enet.sse.exp, enet.rmse.exp, enet.mae.exp)
)

kable(model.results, format = "markdown")
relative_fit = enet.pseudo_r_squared / tree.pseudo_r_squared
print(relative_fit)
if (relative_fit < 1) {
  cat("The regression tree model is better suited to predicting house price")
} else if (relative_fit > 1) {
  cat("The elastic net model is better suited to predicting house price")
} else {
  cat("The models are equally suitable for predicting house price")
}
```

As seen in the metrics above, the Regression Tree performs much better in terms of R-Squared than the Elastic Net with values of ~0.75 vs ~0.60, respectively. The relative fit between elastic net and regression tree was 1.26, which suggests that the Elastic Net model is a much better fit for the data. Both models have similar RMSE for their prediction error.

## Diagnostic plots

```{r}
library(ggplot2)
library(dplyr)

tree.residuals <- tree.actual - tree.predicted
enet.residuals <- enet.actual - enet.predicted
exp.enet.residuals <- enet.actual.exp - exp(enet.predicted)

# Create data frames for visualization
tree_residuals_plot <- data.frame(
  Index = seq_along(tree.actual),
  Residuals = tree.residuals
)

enet_residuals_plot <- data.frame(
  Index = seq_along(enet.actual),
  Residuals = enet.residuals
)

exp_enet_residuals_plot <- data.frame(
  Index = seq_along(enet.actual.exp),
  Residuals = exp.enet.residuals
)

# Create scatterplots for each model
tree_plot <- ggplot(tree_residuals_plot, aes(x = Index, y = Residuals)) +
  geom_point() +
  labs(title = "Regression Tree Residuals",
       x = "Index",
       y = "Residuals") +
  theme_minimal()

enet_plot <- ggplot(enet_residuals_plot, aes(x = Index, y = Residuals)) +
  geom_point() +
  labs(title = "Elastic Net Residuals",
       x = "Index",
       y = "Residuals") +
  theme_minimal()

exp_enet_plot <- ggplot(exp_enet_residuals_plot, aes(x = Index, y = Residuals)) +
  geom_point() +
  labs(title = "Elastic Net Residuals",
       x = "Index",
       y = "Residuals") +
  theme_minimal()

tree.boxplot <- ggplot(data.frame(Residuals = tree.residuals), aes(x = "", y = Residuals)) +
  geom_boxplot(fill = "lightblue", color = "blue") +
  labs(title = "Tree Model Residuals",
       x = "",
       y = "Residuals") +
  theme_minimal()

enet.boxplot <- ggplot(data.frame(Residuals = exp.enet.residuals), aes(x = "", y = Residuals)) +
  geom_boxplot(fill = "lightblue", color = "blue") +
  labs(title = "Elastic Net Residuals",
       x = "",
       y = "Residuals") +
  theme_minimal()

par(mfrow = c(2,2))
# Plot the box plots
print(tree.boxplot)
print(enet.boxplot)
print(tree_plot)
print(exp_enet_plot)
```

It was noted previously that there were problems with heteroskedacity and non-normality in the data. The residuals do appear roughly normal for both models in the plots above. Regardless, we do not expect that any of the assumptions made in standard linear regressions to have a major impact on these models. Elastic net contains regularization parameters to mitigate the impact of overfitting to imperfect data and regression trees do not hold these assumptions.

Based on the performance of their performance with the test data, the two models attain a similar level of prediction error. Despite this, the Regression Tree has a much larger Pseudo R-Squared value suggesting that it is better suited for this data. One issue to consider is the RMSE of 2.102e+05 and 2.281+05 which will impact interpretation of price data on the low end. Another point would be that the data used to train these models only contains data from two years and for two specific counties. Model developed on this data set may lack external validity.

\newpage
# VII. Ongoing Model Monitoring Plan (5 points) {#section7}

## Overview

For model monitoring there are four areas to track:

1) Model stability 
- e.g. Comparing prediction errors and mean and variance of predicted values on a new batch of data to older batches
- Normality testing on residuals from a new batch of predictions

2) Prediction performance 
- e.g. Comparing pseudo R2 and MSE values of our baseline and production models with historical data over time; strong fluctuations in the production model performance compared to baseline model suggests quality issues with the production model

3) Incremental data quality
- e.g. monitor for outliers/ influential points in incoming new data using tools like Residuals vs Leverage plot and Cook's distance

4) data pipeline failures 

Below we will provide more details and concrete examples on how the model monitoring plan will be implemented based on our team member's practical experience.

## 1. Prediction Stability

This subsection concerns itself with stability of predictions. Price predictions often are used by loan originators to size a loan or as an input to a consumer product. For example, assume the product is a home price recommendation for homeowners looking to see. If the homeowners aka the customers get highly variable sale price recommendations month to month, it'll result in subpar user experience, creating distrust between the customer and the firm. 

To check for prediction stability the approach is to store last month's prediction values in a database table and the model itself in an S3 bucket as an RDS file. We are assuming new data arrives in monthly batches because that is the frequency of our data set.

```{r echo=TRUE}
# Helper function to define appropriate window for "old data" as baseline for comparison to new batch
adjust_months <- function(date, num_months) {
  # Convert to character for easier manipulation
  date_str <- as.character(date)
  
  # Extract year and month components
  year <- substr(date_str, 1, 4)
  month <- substr(date_str, 5, 6)
  
  # Convert to numeric and adjust months
  result_month <- as.numeric(month) + num_months
  result_year <- as.numeric(year) + floor((result_month - 1) / 12)
  result_month <- (result_month - 1) %% 12 + 1  # Ensure the month is within 1 to 12
  
  # Create the result as "yyyymm"
  result <- paste0(result_year, sprintf("%02d", result_month))
  result <- as.double(result)
  
  return(result)
}
```

Since we do not have infrastructure stood up we use the following code block to simulate pulling in previous month's model and corresponding predictions. We sample the data using a sliding window approach. The past 12 months of data are used to train the model and the subsequent month is the test set. This different from the world in previous parts because here we are simulating the arrival of new data.

```{r section7previousmodel}
# Simulate model training and performance test on existing batch of data
# (In this example, we use data from 2014/01-2015/01 for training and 2015/01-2015/02 for testing) 
concat_date <- year*100 + month
kc.house.df$yyyymm <- concat_date

set.seed(1023)

lookback.window <- 12 #one year lookback window
#Any data before this date is in the training set
#Any data after is in the test set
cutoff_date <- 201501
train_start <- adjust_months(cutoff_date, -lookback.window)

prev_train_data <- kc.house.df[
  which(
    kc.house.df$yyyymm < cutoff_date &
    kc.house.df$yyyymm >= train_start
  ), 
]

#horizon is the size of the test set
#Here one means the subsequent month, so 201501
#If it was 2 then the test set will include [201501-201502] so forth
horizon <- 1
test_start <- cutoff_date
test_end <- adjust_months(test_start, horizon)

prev_test_data <- kc.house.df[
  which(
    (kc.house.df$yyyymm < test_end) &
    (kc.house.df$yyyymm >= test_start) 
  ), 
]
prev_train_data <- prev_train_data %>% dplyr::select(-contains("Month"))
prev_test_data <- prev_test_data %>% dplyr::select(-contains("Month"))

previous.model <- lm(price ~ . -year, data = prev_train_data)
previous.pred <- predict(previous.model, prev_test_data)
```

Now we simulate getting a new batch of data and fitting the model on new data. We compare the new model predictions against the old model using 

1. T-test on the means of the predictions
  
  - To determine if prediction means are drifting from month to month

2. F-test for the variance of the predictions

  - To determine if prediction variance is drifting from month to month

3. T-test on the mean squared error. 

  - To determine if mean squared error is drifting from month to month

For assumption checking, we use Anderson-Darling test to check normality of predictions. Shapiro-Wilk test will not work due to the large sample size.

In practice, model stability tests tend to be overly sensitive so we will trigger Slack warnings when the p-values of the test are less than 0.01. 

```{r}
# Simulate arrival of new data and compare this month's prediction performance to last month's
library(nortest)

#Current Model
cutoff_date <- adjust_months(cutoff_date, 1)
train_start <- adjust_months(cutoff_date, -lookback.window)

curr_train_data <- kc.house.df[
  which(
    kc.house.df$yyyymm < cutoff_date &
    kc.house.df$yyyymm >= train_start
  ), 
]

horizon <- 1
test_start <- cutoff_date
test_end <- adjust_months(test_start, horizon)

curr_test_data <- kc.house.df[
  which(
    (kc.house.df$yyyymm < test_end) &
    (kc.house.df$yyyymm >= test_start) 
    
  ), 
]
curr_train_data <- curr_train_data %>% dplyr::select(-contains("Month"))
curr_test_data <- curr_test_data %>% dplyr::select(-contains("Month"))

current.model <- lm(price ~ . -year, data = curr_train_data)
curr.pred <- predict(current.model, curr_test_data)

# H0: same means
# H1: means are not the same
t.test(previous.pred, curr.pred)

# H0: same variance
# H1: variance are not the same
var.test(previous.pred, curr.pred)

#Check if mean squared error is equivalent
prev_sq_error <- (prev_test_data[, "price"] - previous.pred)^2
curr_sq_error <- (curr_test_data[, "price"] - curr.pred)^2
t.test(prev_sq_error, curr_sq_error)

#Check predictions are normal
#H0: Data is normally distributed
#H1: Data is not normally distributed
ad.test(current.model$residuals)
```

Looks like in our case, the tests will trigger and warn us that the variance of the predictions have deviated (increased), means have not, MSE has not and the AD test will trigger an assumption violation alert. Variance of predictions deviating is worth notifying because we want to ensure stakeholders and customers a consistent experience. 

The most important of these is MSE as we do not want our model to fluctuate in terms of error. We certainly do not want MSE to deviate downwards to significant degree (alert can adjusted to trigger when difference is negative and significant). Normality is important for inference in the case that model coefficients become part of a product. However it is not as important currently since we are only concerned with predictive power.

## Prediction Performance over Time

For this section we track model metrics over time to ensure the model continues to be on par. Metrics we choose are adjusted $R^2$ and RMSE. We apply a function to create a rolling window for the model to train on and then test with data outside the window. We collect the metrics and plot them over time. 

If any metrics of the production model is worse than that of baseline production model, we will trigger a slack message.

```{r}
library(ggplot2)

#Iterate over folds
roll_model <- function(cutoff_date, form){
  
  lookback.window <- 6 #six month lookback
  train_start <- adjust_months(cutoff_date, -lookback.window)
  
  train_data <- kc.house.df[
    which(
      kc.house.df$yyyymm < cutoff_date &
      kc.house.df$yyyymm >= train_start
    ), 
  ]
  train_data <- train_data %>% dplyr::select(-contains("Month"))
  
  horizon <- 1
  test_start <- cutoff_date
  test_end <- adjust_months(test_start, horizon)
  
  test_data <- kc.house.df[
    which(
      (kc.house.df$yyyymm < test_end) &
      (kc.house.df$yyyymm >= test_start) 
      
    ), 
  ]
  test_data <- test_data %>% dplyr::select(-contains("Month"))
  
  model <- lm(form, data = train_data)
  pred <- predict(model, test_data)
  
  act <- test_data[, "price"]
  n <- dim(model.matrix(model))[1]
  p <- dim(model.matrix(model))[2]
  
  metric <- CalcTestMetrics(pred, act, n, p)

  return (metric)
}

years <- c(201408, 201409, 201410, 201411, 201412, 201501, 201502, 201503)

ols_form <- as.formula("price ~ . -year")
metrics_table <- do.call(rbind, lapply(years, roll_model, form = ols_form))
metrics_table <- as.data.frame(metrics_table)
metrics_table$years <- years
metrics_table$rmse <- sqrt(as.numeric(metrics_table$mse))
metrics_table$model.name <- "Production Model"

#assume this it the baseline model
null_form <- as.formula("price ~ sqft_living")
null_metrics_table <- do.call(rbind, lapply(years, roll_model, form = null_form))
null_metrics_table <- as.data.frame(null_metrics_table)
null_metrics_table$years <- years
null_metrics_table$rmse <- sqrt(as.numeric(null_metrics_table$mse))
null_metrics_table$model.name <- "Baseline Model"



metrics_table <- dplyr::union(metrics_table, null_metrics_table)

# Convert date to Date class
metrics_table$date <- as.Date(paste0(metrics_table$years, "01"), format = "%Y%m%d")

# Create the ggplot
ggplot(metrics_table, aes(x = date, y = as.numeric(adj.rsquared), color = model.name)) +
  geom_point(size = 4) + 
  labs(x = "Year-Month", y = "Adjusted R Squared", title = "Adjusted R Squared over Months") +
  scale_x_date(date_labels = "%Y-%m", date_breaks = "1 month") +
  theme_minimal()

ggplot(metrics_table, aes(x = date, y = as.numeric(rmse), color = model.name)) +
  geom_point(size = 4) +
  labs(x = "Year-Month", y = "RMSE", title = "RMSE over Months") +
  scale_x_date(date_labels = "%Y-%m", date_breaks = "1 month") +
  theme_minimal()
```

In both the `Adjusted R Squared over Months` and `RMSE over Months` plots, the production model is working much better than baseline model as we expect, so no alerts will be triggered.

## Incremental Data Quality Check

When new data comes in at monthly interval we will perform checks to prevent malformed data from entering the model. First we create graphs to add to a dashboard for visual inspection. These are the `Residuals vs Leverage` plot to detect outliers, leverage points or both and the `Cook's Distance` chart to detect influential points. The hope is to catch problematic points early, before they reach the model.

```{r}
library(olsrr)
incremental_data <- curr_test_data
incremental.model <- lm(price ~ . -year, data = incremental_data)

#Residual vs Leverage
ols_plot_resid_lev(incremental.model)
#Cook's Distance Visualized
ols_plot_cooksd_chart(incremental.model)
```

To compare the old data with the new incremental data batch we perform a Kolmogorov-Smirnov test for continuous features and Chi-Square Goodness of Fit test for categorical data. The threshold we set will be a conservative 0.01 because these tests tend to be overly sensitive due to the variable nature of real data.

```{r}
old_data <- curr_train_data

continuous_features <- c(
  "price", 
  "sqft_living", 
  "sqft_lot", 
  "sqft_above", 
  "year_built", 
  "yr_renovated", 
  "sqft_living15", 
  "sqft_lot15"
)
discrete_features <- c(
  "bedrooms", 
  "bathrooms", 
  "floors", 
  "waterfront", 
  "view", 
  "condition", 
  "grade", 
  "zipcode_start"
)

#H0: Same Distribution
#H1: Not same distribution
ks_pvalues <- c()
for (feature in continuous_features){
  pval <- ks.test(
    old_data[, feature], 
    incremental_data[, feature], 
    simulate.p.value = TRUE
  )$p.value
  
  ks_pvalues <- c(ks_pvalues, pval)
}

#H0: Same Proportions
#H1: Not same proportions
chisq_pvalues <- c()
for (feature in discrete_features){
  # Extract the column data
  old_feature_data <- old_data[[feature]]
  n <- length(old_feature_data)
  expected_probabilities <- table(old_feature_data) / n 
  
  incremental_feature_data <- incremental_data[[feature]]
  n <- length(incremental_feature_data)
  observed_probabilities <- table(old_feature_data) / n
  
  #want to compare new to old
  pval <- chisq.test(
    observed_probabilities, 
    expected_probabilities, 
    simulate.p.value = TRUE
  )$p.value
  
  chisq_pvalues <- c(chisq_pvalues, pval)
}

cbind(continuous_features, ks_pvalues)
cbind(discrete_features, chisq_pvalues)
```

Looks like `sqft_living`, `bedrooms`, `bathrooms` and deviated in our incremental data batch.

## Pipeline Fail Safes

*Note: No examples are shown here because we do not have data infrastructure*

The plan to handle pipeline failures is to persist every production model and every test and training set of data. It is ok to store large amounts of data because storage is cheap in the modern day. Models RDS files will be stored in an S3 bucket. The data sets will be stored as compressed parquet files because production database tables usually only contain the most updated version (upserted records) and in this case we want to revert to data before any updates were made. When the pipeline fails, we can use old data and the old model to continue to generate predictions. This ensures downstream stakeholders are free from breaking changes and free from work disruption.

![Model Monitoring Architecture](img/model_monitoring.png)

\newpage
# VIII. Conclusion (5 points) {#section8}

This project aims to develop statistical models for predicting house prices in Kings County, USA. Using our training dataset, we built seven different linear regression models and compared them against each other, with elastic net regression achieving the highest adjusted R-squared (0.759) and lowest MSE. We developed alternative models using regression tree, which achieved an adjusted R-squared value of 0.65. 

Below we show the performance and coefficient comparisons between linear models again. Aside from baseline and stepwise selection models (which are not log-transformed), all other linear models show very comparable performance. We believe that the lasso regression model is our champion model. Its high adjusted R-squared value means that it can explain over 75% of the variation in house prices seen in the dataset, and its low MSE value indicates its accuracy when predicting house prices in the test dataset. 

Our simple linear regression analysis found that the variables with most significant p-values are `bathrooms`, `sqft_living`, `view`, `year_built`, `yr_renovated`, `lat`, `sqft_living15`, the quality adjusted measures. All of these variables except year built is positively correlated with price. An interesting insight is that the timing of the house sale also matters, because purchases being made in January and February has a negative effect on house prices, suggesting that there is seasonality.

```{r}
knitr::kable(sorted_results_df, caption = "Model Metrics")
knitr::kable(dplyr::select(all_models, -c(ElasticNetNoTransform, Baseline, Stepwise)), caption = "Model Coefficients (showing log-transformed models only)")
```

Our regression tree model also highlighted a set of variables with highest importance, which includes grade, sqft_living, sqft_above, sqft_living15, bathrooms and latitude. Both regression tree and linear regression models agree that higher latitude may predict higher house prices, suggesting that perhaps some less desirable areas correspond with lower latitude. In both types of models, higher grade (quality of construction and design) and higher number of bathrooms also associates with higher house prices. In contrast, most of the square footage variables relevant to regression tree did not show strong significance in the linear models. It is possible that the relationship between square footage with price are non-linear and better captured by a non-parametric model like the regression tree. All the square footage-related metrics also likely run into multicollinearity issues with linear regression.

In conclusion, we propose a final elastic net regression model that could predict house sales prices based on commonly measured variables. We believe such a model can be a helpful tool for consumers and real estate service providers to estimate the value of future properties on the market, and for them to understand significant factors that influence home values.

\newpage
## Bibliography (7 points)

*Please include all references, articles and papers in this section.*

1. McGill University
- http://www.med.mcgill.ca/epidemiology/joseph/courses/EPIB-621/centered_var.pdf

2. Zip Code List in Washington
https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?zip=980&stfips=&state=wa&stname=washington

3. Anderson Darling Test
- https://www.rdocumentation.org/packages/nortest/versions/1.0-4/topics/ad.test

4. ggplot2
- https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf

5. CSCI E-106 Homework 7 Solutions
- Cloud/project/Homework Solutions/

6. dispRegFunc()
- Rafael Gomez

7. CSCI E-106 Homework 9 Solutions
- Cloud/project/Homework Solutions/

8. Kolmogorov-Smirnov Test
- https://www.rdocumentation.org/packages/dgof/versions/1.4/topics/ks.test
- https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test

9. Feature importance plot
- https://bookdown.org/mpfoley1973/data-sci/classification-tree.html

Lastly, we would like to express our sincere appreciation for the instructors and teaching assistants of CSCI E-106 for sharing their knowledge and support for this project.

## Appendix (3 points)

*Please add any additional supporting graphs, plots and data analysis.*

Additional plots and details from stepwise variable selection.
```{r, fig.width=7}
stepwise_model
plot(stepwise_model)
```

### Ridge Regression

Additional plot showing coefficients at different lambda values for ridge regression.

```{r}
plot(glmnet(x,y,alpha=0,lambda.min.ratio=0.00001),xvar="lambda",label=T)
abline(v=log(best_lambda_ridge), col="blue") # Indicates our lambda value

```

Additional plot showing cross-validation selection for best lambda for ridge regression.

```{r}
cv.ridge <- cv.glmnet(x, y, alpha=0, nlambda=100, lambda.min.ratio=0.0001)
plot(cv.ridge)
```

### Lasso

Additional plot showing coefficients and number of variables at different lambda values for lasso regression.

```{r}
plot(glmnet(x,y,alpha=1,lambda.min.ratio=0.00001),xvar="lambda", label=T)
abline(v=log(best_lambda_lasso), col="blue") # Indicates our lambda value
```

Additional plot showing cross-validation selection for best lambda for lasso regression.

```{r}
cv.lasso <- cv.glmnet(x, y, alpha=1, nlambda=100, lambda.min.ratio=0.00001)
plot(cv.lasso)
```
```
